{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9795515,"sourceType":"datasetVersion","datasetId":6002932},{"sourceId":9796454,"sourceType":"datasetVersion","datasetId":6003628},{"sourceId":221721,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":189117,"modelId":211123}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installing datasets for Hugging face\n!pip install --upgrade pip\n!pip install datasets torch torchvision transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:24:01.066802Z","iopub.execute_input":"2025-01-08T20:24:01.067713Z","iopub.status.idle":"2025-01-08T20:24:30.586321Z","shell.execute_reply.started":"2025-01-08T20:24:01.067671Z","shell.execute_reply":"2025-01-08T20:24:30.585030Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:11.655451Z","iopub.execute_input":"2025-01-08T20:29:11.655944Z","iopub.status.idle":"2025-01-08T20:29:17.080099Z","shell.execute_reply.started":"2025-01-08T20:29:11.655902Z","shell.execute_reply":"2025-01-08T20:29:17.078779Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# importing packages\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom datasets import load_dataset, load_from_disk\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split, DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n# import tqdm\nfrom torchvision import transforms\nfrom torchvision.tv_tensors import BoundingBoxes, Image\nfrom PIL import Image as PILImage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:21.572847Z","iopub.execute_input":"2025-01-08T20:29:21.573265Z","iopub.status.idle":"2025-01-08T20:29:21.581650Z","shell.execute_reply.started":"2025-01-08T20:29:21.573231Z","shell.execute_reply":"2025-01-08T20:29:21.580552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the CelebA dataset from Hugging Face\n# https://huggingface.co/datasets/hfaus/CelebA_bbox_and_facepoints\ndataset = load_dataset(\"hfaus/CelebA_bbox_and_facepoints\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:22.562560Z","iopub.execute_input":"2025-01-08T20:29:22.563056Z","iopub.status.idle":"2025-01-08T20:29:23.144886Z","shell.execute_reply.started":"2025-01-08T20:29:22.562989Z","shell.execute_reply":"2025-01-08T20:29:23.143434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class CelebADataset(Dataset): #v1\n#     def __init__(self, dataset, image_size=(224, 224)):\n#         self.dataset = dataset\n#         self.image_size = image_size\n#         self.transform = transforms.Compose([\n#             transforms.Resize(self.image_size),  # Resize images to 224x224\n#             transforms.ToTensor(), # Normalize from 0 --> 255 to 0 --> 1 while converting to Tensor\n#             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize by centring the data around origin and scale the values\n#         ])\n\n#     def __len__(self):\n#         return len(self.dataset)\n\n#     def __getitem__(self, idx):\n#         example = self.dataset[idx]\n#         image = example['image']\n#         bbox = example['bbox']\n#         landmarks = example['facial_landmarks']\n\n#         # Original image size\n#         orig_width, orig_height = image.size\n\n#         # Apply the image transformations (resize)\n#         image = self.transform(image)\n\n#         # Scale bbox and landmarks to match the resized image\n#         scale_x = self.image_size[0] / orig_width\n#         scale_y = self.image_size[1] / orig_height\n\n#         # Adjust bbox\n#         bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]\n\n#         return image, torch.tensor(bbox, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:25.098132Z","iopub.execute_input":"2025-01-08T20:29:25.099176Z","iopub.status.idle":"2025-01-08T20:29:25.104186Z","shell.execute_reply.started":"2025-01-08T20:29:25.099136Z","shell.execute_reply":"2025-01-08T20:29:25.103003Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torchvision.transforms import v2 as T\n# import torchvision.transforms.functional as TVF\n# class CelebADataset(Dataset): # v4\n#     def __init__(self, dataset, image_size=(224, 224)):\n#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#         self.dataset = dataset\n#         self.image_size = image_size\n#         self.transforms_cpu = T.Compose([\n#             # T.RandomApply([T.RandomHorizontalFlip()], p=0.5),  # 50% chance\n#             # T.RandomApply([T.RandomRotation(degrees=45)], p=0.5), # 50% chance\n#             T.RandomApply([T.JPEG(quality=(70, 100))], p=0.5) # 50% chance\n#         ])\n#         self.transforms = T.Compose([\n#             T.RandomApply([T.RandomHorizontalFlip()], p=0.5),  # 50% chance\n#             T.RandomApply([T.RandomRotation(degrees=45)], p=0.5), # 50% chance\n#             T.RandomApply([T.RandomAffine(degrees=(-15, 15), translate=(0.1, 0.1), scale=(0.5, 1.5), shear=(-10, 10, -10, 10), fill=tuple(random.randint(0, 255) for _ in range(3)))], p=0.5), # 50% chance\n#             T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.5), # 50% chance\n#             # T.RandomApply([T.JPEG(quality=(70, 100))], p=0.5), # 50% chance\n#             # T.RandomApply([T.Lambda(lambda img: T.JPEG(quality=(70, 99))(img.to(torch.device('cpu'))).to(self.device) )], p=0.5), # 50% chance\n#             T.RandomApply([T.ColorJitter(brightness=0.2, contrast=0.2, saturation=(0.8, 1.2), hue=(-0.1, 0.1))], p=0.5), # 50% chance\n#             T.RandomApply([T.Grayscale(num_output_channels=3)], p=0.2), # 20% chance\n#             T.Resize(self.image_size, antialias=True), #these are always applied\n#             T.ToDtype(torch.float32, scale=True),\n#             T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n#         ])\n\n#     def __len__(self):\n#         return len(self.dataset)\n\n#     def __getitem__(self, idx):\n#         example = self.dataset[idx]\n#         image = example['image']\n#         bbox = example['bbox']  # x, y, w, h\n\n#         # Convert to tv_tensors\n#         image = Image(TVF.pil_to_tensor(image))\n#         bbox = BoundingBoxes(torch.tensor([bbox]), format='xywh', canvas_size=image.shape[-2:]) # Important: Provide canvas_size\n\n#         # print(f\"Image shape: {image.shape}\")\n#         # print(f\"BBox shape: {bbox.shape}\")\n\n#         # do the cpu augmentations first\n#         transformed_cpu = self.transforms_cpu(image, bbox)\n#         image = transformed_cpu[0]\n#         bbox = transformed_cpu[1] # Extract the bbox, as it is returned as a list of bboxes\n\n#         # print(f\"Transformed Image shape: {image.shape}\")\n#         # print(f\"Transformed BBox shape: {bbox.shape}\")\n\n#         # use gpu if availble to speed up augmentations\n#         image = image.to(self.device)\n#         bbox = bbox.to(self.device)\n\n#         # print(f\"after to device Image shape: {image.shape}\")\n#         # print(f\"after to device BBox shape: {bbox.shape}\")\n\n#         # Apply transforms\n#         transformed = self.transforms(image, bbox) # Apply transforms to both simultaneously\n#         image = transformed[0]\n#         bbox = transformed[1][0] # Extract the bbox, as it is returned as a list of bboxes\n\n#         # print(f\"Transformed Image shape: {image.shape}\")\n#         # print(f\"Transformed BBox shape: {bbox.shape}\")\n\n#         return image, bbox\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-01-08T20:29:25.395782Z","iopub.execute_input":"2025-01-08T20:29:25.396369Z","iopub.status.idle":"2025-01-08T20:29:25.402904Z","shell.execute_reply.started":"2025-01-08T20:29:25.396333Z","shell.execute_reply":"2025-01-08T20:29:25.401599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2 # Added import here\ndef get_train_transforms(): # v2\n    return A.Compose([\n        A.Resize(224, 224),  # Resize images to 224x224\n        A.RandomBrightnessContrast(p=0.5),  # Randomly change brightness and contrast\n        A.GaussianBlur(p=0.5),  # Apply Gaussian blur\n        A.GaussNoise(p=0.5),  # Add Gaussian noise\n        A.ImageCompression(quality_lower=30, quality_upper=70, p=0.5),  # Simulate compression artifacts\n        A.HueSaturationValue(p=0.5),  # Randomly change hue, saturation, and value\n        A.MotionBlur(p=0.5),  # Apply motion blur\n        A.ToGray(p=0.5),  # Convert image to grayscale\n        A.HorizontalFlip(p=0.5),  # Horizontally flip the image\n        A.RandomRotate90(p=0.5),  # Rotate the image by 90 degrees\n        # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),  # Randomly shift, scale, and rotate (problem)\n        A.Affine(scale=(0.9, 1.1), translate_percent=(0.1, 0.1), rotate=(-15, 15), p=0.5), # Try Uncommenting this and run and see it is working\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),  # Normalize the image\n        ToTensorV2(),  # Convert image to PyTorch tensor\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['labels'], clip=True, min_area=1.0, min_visibility=0.02))\n\ndef get_minimal_transforms():\n    return A.Compose([\n        A.Resize(224, 224),  # Resize images to 224x224\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),  # Normalize the image\n        ToTensorV2(),  # Convert image to PyTorch tensor\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['labels'], clip=True, min_area=1.0, min_visibility=0.02))\n\n\nclass CelebADataset(Dataset):\n    def __init__(self, dataset, transforms=None, minimal_transforms=None):\n        self.dataset = dataset\n        self.transforms = transforms\n        self.minimal_transforms = minimal_transforms\n\n    def __len__(self):\n        return len(self.dataset)\n\n\n    def clamp_bboxes(self, bboxes, hight, width, margin=1):\n      \"\"\" \n      Clamps the bounding boxes to ensure they are within the image boundaries.\n      Args:\n        bboxes: A list of bounding boxes in the coco format [x_min, y_min, bbox_hight, bbox_width].\n        hight: The height of the image.\n        width: The width of the image.\n      Returns:\n        A list of clamped bounding boxes.\n      \"\"\"\n\n      clamped_bboxes = []\n\n      for bbox in bboxes:\n          x_min, y_min, bbox_hight, bbox_width = bbox\n\n          x_max = x_min + bbox_width\n          y_max = y_min + bbox_hight\n\n          x_min = max(margin, x_min)\n          y_min = max(margin, y_min)\n          x_max = min(width - margin, x_max)\n          y_max = min(hight - margin, y_max)\n\n          bbox_hight = y_max - y_min\n          bbox_width = x_max - x_min\n\n          clamped_bboxes.append([x_min, y_min, bbox_hight, bbox_width])\n\n      return clamped_bboxes\n\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        image = np.array(example['image'])\n        bboxes = [example['bbox']]\n        labels = [0]  # Dummy label; adjust as necessary for your use case\n\n        if len(bboxes) == 0:\n            bboxes = [[1, 1, 1, 1]]\n\n        original_height, original_width = image.shape[:2]\n\n        bboxes = self.clamp_bboxes(bboxes, original_height, original_width, margin=1)\n\n        if self.transforms:\n            augmented = self.transforms(image=image, bboxes=bboxes, labels=labels)\n            image = augmented['image']\n            bboxes = augmented['bboxes']\n\n        bboxes = self.clamp_bboxes(bboxes, 224, 224, 1)\n\n        if len(bboxes) == 0:\n            bboxes = [[1, 1, 1, 1]]\n\n        return image, torch.tensor(bboxes[0], dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:26.179105Z","iopub.execute_input":"2025-01-08T20:29:26.179481Z","iopub.status.idle":"2025-01-08T20:29:26.195958Z","shell.execute_reply.started":"2025-01-08T20:29:26.179447Z","shell.execute_reply":"2025-01-08T20:29:26.194994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset into train, test, and validation\ntrain_data = dataset['train']\n# Turns out that test and validation data are corrupt :)               ...   <^_^>\n\n# Create dataset instances for each split\n# train_dataset = CelebADataset(train_data) # v1\n\ntrain_transforms = get_train_transforms() # v2\nminimal_transforms = get_minimal_transforms()\ntrain_dataset = CelebADataset(train_data, transforms=train_transforms, minimal_transforms=minimal_transforms)\n\n# train_dataset = CelebADataset(train_data) # v3\n\n\n# Define the desired sizes for each split\ntrain_size = len(train_dataset) - 10000  # Original train size minus test and val sizes\ntest_size = 5000\nval_size = 5000\n\n# Split the original training dataset\ntrain_dataset, test_dataset, val_dataset = random_split(\n    train_dataset, [train_size, test_size, val_size],\n    generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility\n)\n\n# DataLoader for batch processing /// , num_workers=4, pin_memory=True is for gpu\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:29.927153Z","iopub.execute_input":"2025-01-08T20:29:29.927522Z","iopub.status.idle":"2025-01-08T20:29:29.968942Z","shell.execute_reply.started":"2025-01-08T20:29:29.927492Z","shell.execute_reply":"2025-01-08T20:29:29.968071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_image_with_bbox_and_landmarks_resized(dataset_instance, index):\n  image, bbox = dataset_instance[index]\n  image = image.permute(1, 2, 0).numpy()  # Convert to numpy and correct channel order\n  # Undo the normalization\n  image = (image * 0.5) + 0.5\n\n\n  plt.imshow(image)\n  ax = plt.gca()\n\n  # Draw bounding box using x1, y1, width, height\n  rect = patches.Rectangle(\n      (bbox[0], bbox[1]),  # Using x1, y1 as starting point\n      bbox[2],             # Using width\n      bbox[3],             # Using height\n      linewidth=1,\n      edgecolor='r',\n      facecolor='none'\n  )\n  ax.add_patch(rect)\n\n  plt.show()\n\n# Example usage with the celebA_dataset instance\nsize_dashes = 30\nprint(f\"{'-' * size_dashes} training dataset samples {'-' * size_dashes}\")\nfor i in range(10):\n  r = np.random.randint(0, 162770 - 5000 * 2)\n  show_image_with_bbox_and_landmarks_resized(train_dataset, r)\nprint(f\"{'-' * size_dashes} testing dataset samples {'-' * size_dashes}\")\nfor i in range(10):\n  r = np.random.randint(0, 5000)\n  show_image_with_bbox_and_landmarks_resized(test_dataset, r)\nprint(f\"{'-' * size_dashes} validation dataset samples {'-' * size_dashes}\")\nfor i in range(10):\n  r = np.random.randint(0, 5000)\n  show_image_with_bbox_and_landmarks_resized(val_dataset, r)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:29:42.689405Z","iopub.execute_input":"2025-01-08T20:29:42.690556Z","iopub.status.idle":"2025-01-08T20:29:50.682164Z","shell.execute_reply.started":"2025-01-08T20:29:42.690510Z","shell.execute_reply":"2025-01-08T20:29:50.681095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the model\n# class FaceBBoxLandmarkCNN(nn.Module): v1\n#     def __init__(self):\n#         super(FaceBBoxLandmarkCNN, self).__init__()\n#         # Shared CNN Layers\n#         # torch.Size([3, 224, 224])\n#         # torch.Size([32, 224, 224])\n#         # torch.Size([32, 112, 112])\n#         # torch.Size([64, 112, 112])\n#         # torch.Size([64, 56, 56])\n#         # torch.Size([128, 56, 56])\n#         # torch.Size([128, 28, 28])\n#         # at the end we have 128 channels and 28*28 image\n#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n#         self.bn1 = nn.BatchNorm2d(32)\n#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n#         self.bn2 = nn.BatchNorm2d(64)\n#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n#         self.bn3 = nn.BatchNorm2d(128)\n\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n#         self.dropout_conv = nn.Dropout(0.1)\n#         self.dropout_fc = nn.Dropout(0.15)\n\n#         self.fc_bbox = nn.Sequential(\n#             nn.Linear(128 * 28 * 28, 2048),\n#             nn.ReLU(),\n#             self.dropout_fc,\n#             nn.Linear(2048, 512),\n#             nn.ReLU(),\n#             self.dropout_fc,\n#             nn.Linear(512, 128),\n#             nn.ReLU(),\n#             self.dropout_fc,\n#             nn.Linear(128, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, 4)\n#         )\n\n#     def forward(self, x):\n#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n#         x = self.dropout_conv(x)\n#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n#         x = self.dropout_conv(x)\n#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n#         x = self.dropout_conv(x)\n\n#         x = x.view(-1, 128 * 28 * 28)\n#         bbox_output = self.fc_bbox(x)\n\n#         return bbox_output\n# class FaceBBoxLandmarkCNN(nn.Module): v2\n#     def __init__(self):\n#         super(FaceBBoxLandmarkCNN, self).__init__()\n        \n#         # Convolutional layers with reduced filter sizes\n#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n#         self.bn1 = nn.BatchNorm2d(16)\n#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n#         self.bn2 = nn.BatchNorm2d(32)\n#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n#         self.bn3 = nn.BatchNorm2d(64)\n        \n#         # Pooling layer\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        \n#         # Fully connected layers for bounding box prediction\n#         self.fc_bbox = nn.Sequential(\n#             nn.Linear(64 * 28 * 28, 512),  # Input size matches the flattened conv layer output\n#             nn.ReLU(),\n#             nn.Linear(512, 128),\n#             nn.ReLU(),\n#             nn.Linear(128, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, 4)  # Output: [x1, y1, width, height]\n#         )\n\n#     def forward(self, x):\n#         # Convolutional layers with ReLU, Batch Norm, Pooling, and Dropout\n#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        \n#         # Flatten the output from the convolutional layers\n#         x = x.view(-1, 64 * 28 * 28)\n        \n#         # Bounding box prediction\n#         bbox_output = self.fc_bbox(x)\n        \n#         return bbox_outputA\n# class InvertedResidual(nn.Module):\n#     def __init__(self, inp, oup, stride, expand_ratio):\n#         super(InvertedResidual, self).__init__()\n#         self.stride = stride\n#         assert stride in [1, 2]\n\n#         hidden_dim = int(inp * expand_ratio)\n#         self.use_res_connect = self.stride == 1 and inp == oup\n\n#         layers = []\n#         if expand_ratio != 1:\n#             # Expansion\n#             layers.extend([\n#                 nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n#                 nn.BatchNorm2d(hidden_dim),\n#                 nn.ReLU6(inplace=True)\n#             ])\n        \n#         layers.extend([\n#             # Depthwise\n#             nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n#             nn.BatchNorm2d(hidden_dim),\n#             nn.ReLU6(inplace=True),\n#             # Pointwise\n#             nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n#             nn.BatchNorm2d(oup)\n#         ])\n        \n#         self.conv = nn.Sequential(*layers)\n\n#     def forward(self, x):\n#         if self.use_res_connect:\n#             return x + self.conv(x)\n#         return self.conv(x)\n\n# class FaceBBoxModelV2(nn.Module): v3\n#     def __init__(self, width_mult=1.0):\n#         super(FaceBBoxModelV2, self).__init__()\n#         input_channel = 32\n#         last_channel = 128\n        \n#         # Adjust channels based on width multiplier\n#         input_channel = int(input_channel * width_mult)\n#         last_channel = int(last_channel * width_mult)\n\n#         # Initial conv layer\n#         self.features = [\n#             nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n#             nn.BatchNorm2d(input_channel),\n#             nn.ReLU6(inplace=True)\n#         ]\n\n#         # Inverted Residual blocks\n#         self.features.extend([\n#             InvertedResidual(input_channel, 64, 2, expand_ratio=6),\n#             InvertedResidual(64, 64, 1, expand_ratio=6),\n#             InvertedResidual(64, last_channel, 2, expand_ratio=6),\n#             InvertedResidual(last_channel, last_channel, 1, expand_ratio=6)\n#         ])\n\n#         # Global average pooling and final layers\n#         self.features.extend([\n#             nn.AdaptiveAvgPool2d(1),\n#             nn.Conv2d(last_channel, 64, 1, 1, 0, bias=False),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU6(inplace=True)\n#         ])\n\n#         self.features = nn.Sequential(*self.features)\n\n#         # Lightweight bbox head\n#         self.bbox_head = nn.Sequential(\n#             nn.Linear(64, 4),  # Direct mapping to bbox coordinates\n#             nn.Sigmoid()  # Constrain outputs to [0,1] range\n#         )\n\n#         self._initialize_weights()\n\n#     def _initialize_weights(self):\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out')\n#                 if m.bias is not None:\n#                     nn.init.zeros_(m.bias)\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 nn.init.ones_(m.weight)\n#                 nn.init.zeros_(m.bias)\n#             elif isinstance(m, nn.Linear):\n#                 nn.init.normal_(m.weight, 0, 0.01)\n#                 nn.init.zeros_(m.bias)\n\n#     def forward(self, x):\n#         x = self.features(x)\n#         x = x.view(-1, 64)\n#         bbox = self.bbox_head(x)\n#         return bbox\n# class FaceBBoxModel(nn.Module): # v4\n#     def __init__(self):\n#         super(FaceBBoxModel, self).__init__()\n        \n#         # Efficient feature extraction with depthwise separable convolutions\n#         self.conv1 = nn.Sequential(\n#             nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU6(inplace=True)\n#         )\n        \n#         self.conv2 = nn.Sequential(\n#             # Depthwise\n#             nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU6(inplace=True),\n#             # Pointwise\n#             nn.Conv2d(32, 64, kernel_size=1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU6(inplace=True)\n#         )\n        \n#         self.conv3 = nn.Sequential(\n#             # Depthwise\n#             nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, groups=64),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU6(inplace=True),\n#             # Pointwise\n#             nn.Conv2d(64, 128, kernel_size=1),\n#             nn.BatchNorm2d(128),\n#             nn.ReLU6(inplace=True)\n#         )\n        \n#         # Global average pooling instead of flattening\n#         self.gap = nn.AdaptiveAvgPool2d(1)\n        \n#         # Efficient bbox regression head\n#         self.bbox_head = nn.Sequential(\n#             nn.Linear(128, 64),\n#             nn.ReLU6(inplace=True),\n#             nn.Linear(64, 4)  # x, y, width, height\n#         )\n        \n#         self._initialize_weights()\n    \n#     def _initialize_weights(self):\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#                 if m.bias is not None:\n#                     nn.init.zeros_(m.bias)\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 nn.init.ones_(m.weight)\n#                 nn.init.zeros_(m.bias)\n#             elif isinstance(m, nn.Linear):\n#                 nn.init.normal_(m.weight, 0, 0.01)\n#                 nn.init.zeros_(m.bias)\n    \n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.conv2(x)\n#         x = self.conv3(x)\n#         x = self.gap(x)\n#         x = x.view(-1, 128)\n#         bbox = self.bbox_head(x)\n#         return bbox","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:03.424509Z","iopub.execute_input":"2025-01-08T20:30:03.425459Z","iopub.status.idle":"2025-01-08T20:30:03.440608Z","shell.execute_reply.started":"2025-01-08T20:30:03.425401Z","shell.execute_reply":"2025-01-08T20:30:03.439267Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FaceBBoxModel(nn.Module): # v5\n    def __init__(self):\n        super(FaceBBoxModel, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n\n        # Pooling layer\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully connected layers for bounding box prediction\n        self.fc_bbox = nn.Sequential(\n            nn.Linear(256 * 7 * 7, 512),  # Input size matches the flattened conv layer output\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128, 4)  # Output: [x1, y1, width, height]\n        )\n\n    def forward(self, x):\n        # Convolutional layers with Batch Norm and Pooling\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n\n        # Flatten the output from the convolutional layers\n        x = x.view(-1, 256 * 7 * 7)\n\n        # Bounding box prediction\n        bbox_output = self.fc_bbox(x)\n\n        return bbox_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:07.861505Z","iopub.execute_input":"2025-01-08T20:30:07.862466Z","iopub.status.idle":"2025-01-08T20:30:07.872383Z","shell.execute_reply.started":"2025-01-08T20:30:07.862428Z","shell.execute_reply":"2025-01-08T20:30:07.871380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, path):\n  \"\"\"Saves the model's state dictionary to the specified path.\n\n  Args:\n    model: The PyTorch model to save.\n    path: The path where the model will be saved.\n  \"\"\"\n  torch.save(model.state_dict(), path)\n  print(f\"Model saved to {path}\")\n\ndef load_model(model, path, device):\n    \"\"\"Loads the model's state dictionary from the specified path.\n\n    Args:\n      model: The PyTorch model to load the state dictionary into.\n      path: The path where the model is saved.\n      device: The device to load the model onto (e.g., 'cuda' or 'cpu').\n    \"\"\"\n    state_dict = torch.load(path, map_location=device, weights_only=True)  # Explicitly load the state_dict\n    model.load_state_dict(state_dict)\n    print(f\"Model loaded from {path}\")\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:08.213451Z","iopub.execute_input":"2025-01-08T20:30:08.213874Z","iopub.status.idle":"2025-01-08T20:30:08.220391Z","shell.execute_reply.started":"2025-01-08T20:30:08.213838Z","shell.execute_reply":"2025-01-08T20:30:08.219255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using TPU\n# !pip install cloud-tpu-client==0.10\n# !pip install torch_xla","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:09.770996Z","iopub.execute_input":"2025-01-08T20:30:09.771959Z","iopub.status.idle":"2025-01-08T20:30:09.775814Z","shell.execute_reply.started":"2025-01-08T20:30:09.771918Z","shell.execute_reply":"2025-01-08T20:30:09.774856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Using TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# from torch_xla.distributed.parallel_loader import ParallelLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:10.026315Z","iopub.execute_input":"2025-01-08T20:30:10.026758Z","iopub.status.idle":"2025-01-08T20:30:10.031304Z","shell.execute_reply.started":"2025-01-08T20:30:10.026717Z","shell.execute_reply":"2025-01-08T20:30:10.030159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the model, criterion, optimizer, scheduler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = xm.xla_device()  # Use TPU device uncomment if using TPU\n# train_loader = ParallelLoader(train_loader, [device]).per_device_loader(device)\n# val_loader = ParallelLoader(val_loader, [device]).per_device_loader(device)\n# test_loader = ParallelLoader(test_loader, [device]).per_device_loader(device)\n\nmodel = FaceBBoxModel().to(device)\n# bbox_criterion = nn.MSELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n# Define the criterion, optimizer, and scheduler\nbbox_criterion = nn.SmoothL1Loss()      # lr=0.005\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:10.447363Z","iopub.execute_input":"2025-01-08T20:30:10.447748Z","iopub.status.idle":"2025-01-08T20:30:10.541912Z","shell.execute_reply.started":"2025-01-08T20:30:10.447713Z","shell.execute_reply":"2025-01-08T20:30:10.540944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Already Trained Model\n# model = load_model(model, '/kaggle/working/best_model.pth', device)\nmodel = load_model(model, '/kaggle/input/bbox_model_v5_epoch_8/pytorch/default/1/bbox_v5.pth', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:12.771794Z","iopub.execute_input":"2025-01-08T20:30:12.772220Z","iopub.status.idle":"2025-01-08T20:30:13.079110Z","shell.execute_reply.started":"2025-01-08T20:30:12.772183Z","shell.execute_reply":"2025-01-08T20:30:13.077979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\nclass EarlyStopping:\n    def __init__(self, patience=3, min_delta=0.0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        return self.early_stop\n\n\nbbox_losses = []\n\ndef train_model(model, train_loader, val_loader, optimizer, scheduler, device, bbox_criterion, num_epochs=10, patience=3):\n    train_bbox_loss, val_bbox_loss = [], []\n    early_stopping = EarlyStopping(patience=patience, min_delta=0.1)\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss_bbox = 0.0\n\n        for i, (images, bboxes) in enumerate(train_loader):\n            images, bboxes = images.to(device), bboxes.to(device)  # comment this if you are using tpu\n            optimizer.zero_grad()\n\n            pred_bboxes = model(images)\n            loss_bbox = bbox_criterion(pred_bboxes, bboxes)\n\n            loss_bbox.backward()\n            optimizer.step()\n\n            running_loss_bbox += loss_bbox.item()\n\n            if i % 600 == 0:\n                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i}/{len(train_loader)}], BBox Loss: {loss_bbox.item():.4f}\")\n\n            # TPU memory management\n            # del images, bboxes, pred_bboxes, loss_bbox\n            # xm.mark_step()\n\n        \n        avg_train_loss_bbox = running_loss_bbox / len(train_loader)\n        train_bbox_loss.append(avg_train_loss_bbox)\n\n        model.eval()\n        val_running_loss_bbox = 0.0\n        with torch.no_grad():\n            for val_images, val_bboxes in val_loader:\n                val_images, val_bboxes = val_images.to(device), val_bboxes.to(device) # comment this if you are using tpu\n                val_pred_bboxes = model(val_images)\n                val_loss_bbox = bbox_criterion(val_pred_bboxes, val_bboxes)\n                val_running_loss_bbox += val_loss_bbox.item()\n\n                # del val_images, val_bboxes, val_pred_bboxes, val_loss_bbox\n                # xm.mark_step()\n        \n        avg_val_loss_bbox = val_running_loss_bbox / len(val_loader)\n        val_bbox_loss.append(avg_val_loss_bbox)\n\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train BBox Loss: {avg_train_loss_bbox:.4f}, Validation BBox Loss: {avg_val_loss_bbox:.4f}\")\n\n        scheduler.step(avg_val_loss_bbox)\n\n        if early_stopping(avg_val_loss_bbox):\n            print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n            break\n\n        if early_stopping.counter == 0:\n            save_model(model, f\"/kaggle/working/bbox_v5_augmented_epoch_{epoch + 1}.pth\")\n            print(f\"Model saved at epoch {epoch + 1}.\")\n\n\n    print(\"Training completed.\")\n    return train_bbox_loss, val_bbox_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:14.160917Z","iopub.execute_input":"2025-01-08T20:30:14.161340Z","iopub.status.idle":"2025-01-08T20:30:14.175031Z","shell.execute_reply.started":"2025-01-08T20:30:14.161304Z","shell.execute_reply":"2025-01-08T20:30:14.173746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_bbox_loss, val_bbox_loss = train_model(\n    model, train_loader, val_loader, optimizer, scheduler, device, bbox_criterion, num_epochs=10, patience=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T20:30:17.510967Z","iopub.execute_input":"2025-01-08T20:30:17.511390Z","iopub.status.idle":"2025-01-08T20:32:53.739683Z","shell.execute_reply.started":"2025-01-08T20:30:17.511358Z","shell.execute_reply":"2025-01-08T20:32:53.738231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(bbox_losses) + 1)), bbox_losses, label='BBox Loss')\nplt.xlabel('Batches (Epochs)')\nplt.ylabel('Loss')\nplt.title('BBox Loss During Training')\nplt.title('BBox Loss During Training')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_bbox_loss, label='Training BBox Loss')\nplt.plot(val_bbox_loss, label='Validation BBox Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model():\n    model.eval()\n    test_loss_bbox = 0.0\n    test_loss_landmarks = 0.0\n\n    with torch.no_grad():\n        for images, bboxes in test_loader:\n            # images, bboxes = images.to(device), bboxes.to(device) # comment this if you are using tpu\n\n            # Forward pass\n            pred_bboxes = model(images)\n\n            # Calculate loss directly using tensors\n            loss_bbox = bbox_criterion(pred_bboxes, bboxes)\n\n            test_loss_bbox += loss_bbox.item()  # Get the scalar value of the loss\n\n    print(f\"Test BBox Loss: {test_loss_bbox / len(test_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_image_with_bboxes(image, gt_bbox, pred_bbox):\n    \"\"\"\n    Displays an image with ground truth and predicted bounding boxes.\n\n    Args:\n        image: The image as a NumPy array.\n        gt_bbox: The ground truth bounding box coordinates as [x1, y1, width, height].\n        pred_bbox: The predicted bounding box coordinates as [x1, y1, width, height].\n    \"\"\"\n\n    image = image.transpose(1, 2, 0)\n    # Undo the normalization\n    image = (image * 0.5) + 0.5\n    if image.ndim == 2:\n        image = np.stack((image,) * 3, axis=-1)\n\n    plt.imshow(image)\n    ax = plt.gca()\n\n    # Draw ground truth bounding box in green\n    rect_gt = patches.Rectangle(\n        (gt_bbox[0], gt_bbox[1]),\n        gt_bbox[2],\n        gt_bbox[3],\n        linewidth=1,\n        edgecolor='g',\n        facecolor='none',\n        label='Ground Truth'\n    )\n    ax.add_patch(rect_gt)\n\n    # Draw predicted bounding box in red\n    rect_pred = patches.Rectangle(\n        (pred_bbox[0], pred_bbox[1]),\n        pred_bbox[2],\n        pred_bbox[3],\n        linewidth=1,\n        edgecolor='r',\n        facecolor='none',\n        label='Predicted'\n    )\n    ax.add_patch(rect_pred)\n\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    for images, bboxes in test_loader:\n        # images, bboxes = images.to(device), bboxes.to(device) # comment this if you are using tpu\n        pred_bboxes = model(images)\n        images = images.cpu().numpy()\n        bboxes = bboxes.cpu().numpy()\n        pred_bboxes = pred_bboxes.cpu().numpy()\n        for i in range(len(images)):\n            show_image_with_bboxes(images[i], bboxes[i], pred_bboxes[i])\n        break","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}